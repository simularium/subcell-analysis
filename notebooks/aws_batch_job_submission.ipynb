{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import getpass\n",
    "import numpy as np\n",
    "from preconfig import Preconfig\n",
    "import pandas as pd\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Upload config files to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vary_compress_rate0000',\n",
       " 'vary_compress_rate0001',\n",
       " 'vary_compress_rate0002',\n",
       " 'vary_compress_rate0003',\n",
       " 'vary_compress_rate0004',\n",
       " 'vary_compress_rate0005',\n",
       " 'vary_compress_rate0006']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preconfig class allows us to parse a template file and generate a list of config files.\n",
    "# Two loops puts the generated config files for a given number of repeats in S3.\n",
    "preconfig = Preconfig()\n",
    "path_to_template = '../templates/vary_compress_rate.cym.tpl'\n",
    "configs = preconfig.parse(path_to_template,{})\n",
    "s3_client = boto3.client(\"s3\")\n",
    "bucket = 'cytosim-working-bucket'\n",
    "num_repeats = 5\n",
    "job_names = []\n",
    "buffered = np.empty((len(configs)), dtype=object)\n",
    "for index, config in enumerate(configs):\n",
    "    job_name = config[:-4]\n",
    "    job_names.append(job_name)\n",
    "    for repeat in range(num_repeats):\n",
    "        opened_config = open(config, \"rb\")\n",
    "        config_name = f'{job_name}/config/{job_name}_{repeat}.cym' \n",
    "        s3_client.put_object(Bucket=bucket, Key=config_name, Body=opened_config)\n",
    "job_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2a. Specify job definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_definition_arn = \"job_definition_arn\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2b. Create and register job definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for a job definition\n",
    "from container_collection.batch.register_batch_job import register_batch_job\n",
    "job_definition_name = \"karthikv_cytosim_varycompressrate\"\n",
    "image = \"simularium/cytosim:latest\"\n",
    "vcpus = 1\n",
    "memory = 7000\n",
    "bucket_name = \"s3://cytosim-working-bucket/\"\n",
    "simulation_name = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "account = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make batch job is a function that takes in the parameters below and returns a dictionary that is used to create a batch job. \n",
    "def make_batch_job(\n",
    "    name: str,\n",
    "    image: str,\n",
    "    account: str,\n",
    "    region: str,\n",
    "    user: str,\n",
    "    vcpus: int,\n",
    "    memory: int,\n",
    "    prefix: str\n",
    ") -> dict:\n",
    "    return {\n",
    "    \"jobDefinitionName\": f\"{user}-{name}\",\n",
    "    \"type\": \"container\",\n",
    "    \"containerProperties\": {\n",
    "        \"image\": image,\n",
    "        \"vcpus\": vcpus,\n",
    "        \"memory\": memory,\n",
    "        \"environment\": [\n",
    "            {\"name\": \"SIMULATION_TYPE\", \"value\": \"AWS\"},\n",
    "            {\"name\": \"S3_INPUT_URL\", \"value\": bucket_name},\n",
    "            {\"name\": \"SIMULATION_NAME\", \"value\": simulation_name}\n",
    "        ],\n",
    "        \"jobRoleArn\": f\"arn:aws:iam::{account}:role/BatchJobRole\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "vary_compress_rate0000\n",
      "1\n",
      "vary_compress_rate0001\n",
      "2\n",
      "vary_compress_rate0002\n",
      "3\n",
      "vary_compress_rate0003\n",
      "4\n",
      "vary_compress_rate0004\n",
      "5\n",
      "vary_compress_rate0005\n",
      "6\n",
      "vary_compress_rate0006\n"
     ]
    }
   ],
   "source": [
    "# Creating job definitions with make_batch_job\n",
    "# Submitting job definitions with register_batch_job\n",
    "jobs = np.empty(len(configs))\n",
    "job_definitions = np.empty((len(configs)), dtype=object)\n",
    "for index in range(len(configs)):\n",
    "    print(index)\n",
    "    simulation_name = job_names[index]\n",
    "    print(simulation_name)\n",
    "    job_definition = make_batch_job(f\"cytosim-varycompressrate-dryrun-{str(index)}\", 'simularium/cytosim:latest', account, 'us-west-2', 'karthikv', 1, 7000, 's3://cytosim-working-bucket/')\n",
    "    registered_jd = register_batch_job(job_definition)\n",
    "    job_definitions[index] = registered_jd\n",
    "    \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Submit job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit batch job allows us to submit a batch job with a given job definition and job name.\n",
    "from container_collection.batch.submit_batch_job import submit_batch_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16255a76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vary_compress_rate0004.cym',\n",
       " 'vary_compress_rate0005.cym',\n",
       " 'vary_compress_rate0006.cym']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_configs = configs[-3:]\n",
    "new_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for our batch job [size indicates our desired number of repeats]\n",
    "job_name = \"cytosim-varycompressrate\"\n",
    "user = \"karthikv\"\n",
    "queue = \"general\"\n",
    "size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "cytosim-varycompressrate-completerun-config4\n",
      "5\n",
      "cytosim-varycompressrate-completerun-config5\n",
      "6\n",
      "cytosim-varycompressrate-completerun-config6\n"
     ]
    }
   ],
   "source": [
    "# Loop to submit our batch jobs [index * size for total number of simulations]\n",
    "for index in range(len(new_configs)):\n",
    "    index = index + 4\n",
    "    print(index)\n",
    "    print(f'{job_name}-completerun-config{index}')\n",
    "    submit_batch_job(name=f'{job_name}-completerun-config{index}', job_definition_arn=job_definitions[index],user=user,queue=queue,size=size)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Monitor job status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check job status, print progress bar\n",
    "from container_collection.batch.check_batch_job import check_batch_job\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get results from s3\n",
    "#simulation_name = 's3://cytosim-working-bucket/vary_compress_rate0000/outputs'\n",
    "import boto3\n",
    "# job_arn = 'arn:aws:batch:us-west-2:108503617402:job/3591d595-f11a-40a9-b340-6fd8288aba4f:0'\n",
    "# client = boto3.client(\"batch\")\n",
    "# response = client.describe_jobs(jobs=[job_arn])[\"jobs\"]\n",
    "\n",
    "def read_cytosim_s3_file(bucket_name, file_name):\n",
    "    s3 = boto3.client('s3')\n",
    "    try:\n",
    "        response = s3.get_object(Bucket=bucket_name, Key=file_name)\n",
    "        file_content = response['Body'].read()\n",
    "        return file_content.decode('utf-8').splitlines()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the file: {e}\")\n",
    "#read_file = read_s3_file('cytosim-working-bucket', 'vary_compress_rate0001/outputs/0/objects.cmo')\n",
    "#print(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81570fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_dataframe(fiber_energy_all, fiber_forces_all, suffix = None, rigidity = 0.041):\n",
    "    bending_energies = []\n",
    "    for line in fiber_energy_all:\n",
    "        line = line.strip()\n",
    "        bending_energy = float(line.split()[2])\n",
    "        bending_energies.append(bending_energy)\n",
    "\n",
    "\n",
    "    single_all_lines = fiber_forces_all\n",
    "\n",
    "    timepoints_forces = []\n",
    "    outputs = []\n",
    "    fid=0\n",
    "    for line in single_all_lines:\n",
    "        line = line.strip()\n",
    "        if line.startswith('%'):\n",
    "            if line.startswith('% time'):\n",
    "                time = float(line.split(' ')[-1])\n",
    "                timepoints_forces.append(time)\n",
    "                singles = {}\n",
    "            elif line.startswith('% end'):\n",
    "                df = pd.DataFrame.from_dict(singles, orient = 'index')\n",
    "                outputs.append(df)\n",
    "    #                     fiber_point=0\n",
    "                fid=0\n",
    "                # print 'finished parsing ' + rundir + ' timepoint ' + str(time)\n",
    "        elif len(line.split()) > 0:\n",
    "            print(line.split())\n",
    "            [fiber_id, xpos, ypos, zpos,\n",
    "            xforce, yforce, zforce, segment_curvature] = line.split()\n",
    "    #                 figure out if you're on the first, second fiber point etc\n",
    "            if int(fid)==int(fiber_id):\n",
    "                fiber_point+=1\n",
    "    #                     print(fiber_point)\n",
    "            else:\n",
    "                fiber_point=0\n",
    "                fid+=1\n",
    "    #                     print('id: '+str(fid))\n",
    "            singles[str(fiber_id)+'_'+str(fiber_point)] = {'fiber_id' : int(fiber_id),\n",
    "                                    'xpos': float(xpos), 'ypos' : float(ypos), 'zpos': float(zpos),\n",
    "                                    'xforce' : float(xforce), 'yforce' : float(yforce),\n",
    "                                    'zforce': float(zforce), 'segment_curvature': float(segment_curvature)}\n",
    "\n",
    "    all_outputs = pd.concat(outputs, keys = timepoints_forces,\n",
    "                        names = ['time', 'id'])\n",
    "    # all_outputs = all_outputs.swaplevel('time','id',axis=0).sort_index()\n",
    "    all_outputs['force_magnitude'] = np.sqrt(np.square(all_outputs['xforce']) + \n",
    "                                        np.square(all_outputs['yforce']) +\n",
    "                                        np.square(all_outputs['zforce']))\n",
    "\n",
    "\n",
    "    #  Segment bending energy, in pN nm \n",
    "    all_outputs['segment_energy'] = all_outputs['segment_curvature'] * rigidity * 1000\n",
    "    # fiber_forces_outputs_allruns.append(all_outputs)\n",
    "\n",
    "    save_folder = Path('dataframes')\n",
    "    save_folder.mkdir(exist_ok=True, parents = True)\n",
    "    \n",
    "    file_name = 'actin-forces'\n",
    "\n",
    "    if suffix is not None:\n",
    "        file_name += suffix\n",
    "\n",
    "    all_outputs.to_csv(save_folder/f'{file_name}.csv')\n",
    "\n",
    "    print( 'finished parsing ')\n",
    "    all_outputs.tail()\n",
    "    return all_outputs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a077856",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd466e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from subcell_analysis.cytosim.post_process_cytosim import create_dataframes_for_repeats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebf465a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'cytosim-working-bucket'\n",
    "num_repeats = 5\n",
    "configs = ['vary_compress_rate0006']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfb21524",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57cf85d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing index 0 and repeat 0\n",
      "Saved Output to dataframes/actin-forces0_0.csv\n",
      "Processing index 0 and repeat 1\n",
      "Saved Output to dataframes/actin-forces0_1.csv\n",
      "Processing index 0 and repeat 2\n",
      "Saved Output to dataframes/actin-forces0_2.csv\n",
      "Processing index 0 and repeat 3\n",
      "Saved Output to dataframes/actin-forces0_3.csv\n",
      "Processing index 0 and repeat 4\n",
      "Saved Output to dataframes/actin-forces0_4.csv\n"
     ]
    }
   ],
   "source": [
    "create_dataframes_for_repeats(bucket_name, num_repeats, configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4f89e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from subcell_analysis.compression_workflow_runner import run_workflow\n",
    "from subcell_analysis.compression_workflow_runner import run_workflow, run_metric_calculation\n",
    "from subcell_analysis.compression_analysis import (\n",
    "    COMPRESSIONMETRIC,\n",
    ")\n",
    "\n",
    "segene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4141232d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb5c72d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "subcell-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "d558620a2403be4c2958b608989a9fc85658cf9867c13a1b5b0a1011c0ea0956"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
